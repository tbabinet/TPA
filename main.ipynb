{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "#import nltk\n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On génère les série temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_type_time_series(D):\n",
    "    return np.array([[np.sin(np.divide(2*np.pi*i*t, 64)) for t in range(128)] for i in range(1,D+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_second_type_time_series(D):\n",
    "    fst_type = generate_first_type_time_series(D)\n",
    "    result = []\n",
    "    frac = 1/(D-1)\n",
    "    for i in range(1,D+1):\n",
    "        loc = []\n",
    "        for t in range(1,129):\n",
    "            fst = fst_type[i-1,t-1]\n",
    "            snd=frac*np.sum(np.delete(fst_type[:,t-1], i-1, axis=0))\n",
    "            loc.append(fst+snd)\n",
    "        result.append(loc)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(D, MTS):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    if MTS:\n",
    "        series = generate_second_type_time_series(D)\n",
    "    else : \n",
    "        series = generate_first_type_time_series(D)\n",
    "     \n",
    "    for serie in series :\n",
    "        for i in range(0,64):\n",
    "            data.append(serie[i:i+64])\n",
    "            labels.append(serie[i+64])\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels) \n",
    "    train_data = TensorDataset(th.from_numpy(data).type(th.LongTensor), th.from_numpy(labels))\n",
    "    train_loader = DataLoader(train_data, shuffle=False, batch_size=1)\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_forecaster(nn.Module):\n",
    "    def __init__(self, nb_cells, hidden_size, input_size, rnn_dropout, window_size, conv_size):\n",
    "        super(GRU_forecaster, self).__init__()\n",
    "        \n",
    "        self.nb_cells = nb_cells\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, nb_cells, batch_first = True, dropout=rnn_dropout)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###Par rapport au papier, k=nbr de filtres et m=hidden size\n",
    "        self.convList = nn.ModuleList([nn.Conv1d(1,1,window_size-1) for i in range(conv_size)])#32 fixed in the paper\n",
    "\n",
    "        self.Wa = nn.Linear(hidden_size, conv_size)\n",
    "        self.Wh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Wv = nn.Linear(conv_size, hidden_size)\n",
    "        self.Whp = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "    \n",
    "    def forward(self, inputs, h0):\n",
    "        inputs = inputs.view(1,-1,1).float()\n",
    "        batch_size = inputs.size(0)\n",
    "        rnn_out, h = self.gru(inputs, h0)\n",
    "        hidden = rnn_out.transpose(1,2)[:,:,:-1]\n",
    "        last_h = rnn_out[0,-1]\n",
    "        hc = th.Tensor() #size m*k\n",
    "        #l_out = self.getNbFeatures()\n",
    "        \n",
    "        for conv in self.convList:\n",
    "            t = th.Tensor()\n",
    "            for h in hidden[0]:\n",
    "                h=h.view(1,1,-1)\n",
    "                x = conv(h)\n",
    "                t=th.cat([t,x],0)\n",
    "            t = t.view(-1,1)\n",
    "            hc=th.cat([hc,t],1)\n",
    "        \n",
    "        attention_weights = self.Wa(last_h).view(-1,1)\n",
    "        attention_weights = (hc@attention_weights)\n",
    "        attention_weights = self.sigmoid(attention_weights)\n",
    "        \n",
    "        \n",
    "        vt = (attention_weights*hc).sum(dim=0) #ai * Hci\n",
    "        \n",
    "        \n",
    "        hp = self.Wv(vt)+self.Wh(last_h)\n",
    "        out = self.Whp(hp)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = th.Tensor(self.nb_cells, batch_size, self.hidden_size)\n",
    "        hidden = weight.new(self.nb_cells, batch_size, self.hidden_size).zero_()\n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cells=1\n",
    "learning_rate = 1e-3\n",
    "hidden_size=10\n",
    "input_size=1\n",
    "gru=GRU_forecaster(nb_cells,hidden_size, input_size, 0.0,64,8)\n",
    "loss_fn = nn.L1Loss()\n",
    "D=6\n",
    "loader = get_loader(D, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model,lr, max_epochs=200):\n",
    "    optim = th.optim.Adam(params=model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.train()\n",
    "    for i in range(max_epochs):\n",
    "        n = 0 \n",
    "        h = model.init_hidden(1)\n",
    "        train_mean_loss=0\n",
    "        for x, labels in loader:\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            h = h.data\n",
    "            \n",
    "            n+=1\n",
    "            preds, _ = model(x, h)  \n",
    "            \n",
    "            preds=preds.view(1,-1)\n",
    "            labels=labels.view(1,-1).float()\n",
    "            loss = loss_fn(preds, labels)\n",
    "            h = h.detach()\n",
    "            train_mean_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "        \n",
    "            if(n%20==0):\n",
    "                print(\"step : {}/{} \".format(n, 64*D))\n",
    "                print(\"step loss : \", log(train_mean_loss/n,10))  \n",
    "        \n",
    "#         rnn_train_losses.append(train_mean_loss/len(train_data))\n",
    "        \n",
    "        print(\"EPOCH {}\".format(i+1))\n",
    "        print(\"Train Mean loss : \",log(train_mean_loss/n,10))\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 20/384 \n",
      "step loss :  -0.29687578619458854\n",
      "step : 40/384 \n",
      "step loss :  -0.25624323221986556\n",
      "step : 60/384 \n",
      "step loss :  -0.13633444804153508\n",
      "step : 80/384 \n",
      "step loss :  -0.15881915189131585\n",
      "step : 100/384 \n",
      "step loss :  -0.17442701153654336\n",
      "step : 120/384 \n",
      "step loss :  -0.169878251730591\n",
      "step : 140/384 \n",
      "step loss :  -0.17335948414044772\n",
      "step : 160/384 \n",
      "step loss :  -0.17220572378435461\n",
      "step : 180/384 \n",
      "step loss :  -0.17302098585169556\n",
      "step : 200/384 \n",
      "step loss :  -0.17628895263859395\n",
      "step : 220/384 \n",
      "step loss :  -0.1799926081024672\n",
      "step : 240/384 \n",
      "step loss :  -0.1804707382872843\n",
      "step : 260/384 \n",
      "step loss :  -0.1826815697742884\n",
      "step : 280/384 \n",
      "step loss :  -0.18295744074581302\n",
      "step : 300/384 \n",
      "step loss :  -0.1834637507680376\n",
      "step : 320/384 \n",
      "step loss :  -0.1844696817221297\n",
      "step : 340/384 \n",
      "step loss :  -0.18528470076305625\n",
      "step : 360/384 \n",
      "step loss :  -0.1866912571184839\n",
      "step : 380/384 \n",
      "step loss :  -0.18760244486026026\n",
      "EPOCH 1\n",
      "Train Mean loss :  -0.18658096023208307\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.193688137671822\n",
      "step : 40/384 \n",
      "step loss :  -0.24943799702500563\n",
      "step : 60/384 \n",
      "step loss :  -0.14762365706811026\n",
      "step : 80/384 \n",
      "step loss :  -0.17133108447193618\n",
      "step : 100/384 \n",
      "step loss :  -0.18408005121275645\n",
      "step : 120/384 \n",
      "step loss :  -0.17990901249679647\n",
      "step : 140/384 \n",
      "step loss :  -0.182704148041215\n",
      "step : 160/384 \n",
      "step loss :  -0.1810140660964758\n",
      "step : 180/384 \n",
      "step loss :  -0.18141292255491903\n",
      "step : 200/384 \n",
      "step loss :  -0.18447889058234496\n",
      "step : 220/384 \n",
      "step loss :  -0.18861423827757512\n",
      "step : 240/384 \n",
      "step loss :  -0.19013771892159526\n",
      "step : 260/384 \n",
      "step loss :  -0.19312895446705727\n",
      "step : 280/384 \n",
      "step loss :  -0.19308607466244132\n",
      "step : 300/384 \n",
      "step loss :  -0.19301412221546815\n",
      "step : 320/384 \n",
      "step loss :  -0.19401382719952712\n",
      "step : 340/384 \n",
      "step loss :  -0.1944289491653843\n",
      "step : 360/384 \n",
      "step loss :  -0.19573760174238444\n",
      "step : 380/384 \n",
      "step loss :  -0.19694995675354227\n",
      "EPOCH 2\n",
      "Train Mean loss :  -0.1958041701244723\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.19824122398465946\n",
      "step : 40/384 \n",
      "step loss :  -0.2502231346304387\n",
      "step : 60/384 \n",
      "step loss :  -0.15109858322256958\n",
      "step : 80/384 \n",
      "step loss :  -0.1757400125622641\n",
      "step : 100/384 \n",
      "step loss :  -0.18920028607997907\n",
      "step : 120/384 \n",
      "step loss :  -0.18660812644840596\n",
      "step : 140/384 \n",
      "step loss :  -0.1899612229965826\n",
      "step : 160/384 \n",
      "step loss :  -0.18864260117414147\n",
      "step : 180/384 \n",
      "step loss :  -0.18932040969182395\n",
      "step : 200/384 \n",
      "step loss :  -0.19356913703121809\n",
      "step : 220/384 \n",
      "step loss :  -0.20309252069119052\n",
      "step : 240/384 \n",
      "step loss :  -0.21763243902113133\n",
      "step : 260/384 \n",
      "step loss :  -0.23804315039344395\n",
      "step : 280/384 \n",
      "step loss :  -0.2345811301743613\n",
      "step : 300/384 \n",
      "step loss :  -0.23355670974173043\n",
      "step : 320/384 \n",
      "step loss :  -0.23304191976401709\n",
      "step : 340/384 \n",
      "step loss :  -0.2329551437704766\n",
      "step : 360/384 \n",
      "step loss :  -0.23651732377106982\n",
      "step : 380/384 \n",
      "step loss :  -0.2437356646048503\n",
      "EPOCH 3\n",
      "Train Mean loss :  -0.24446169480624586\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.2587693978641648\n",
      "step : 40/384 \n",
      "step loss :  -0.25211906806665657\n",
      "step : 60/384 \n",
      "step loss :  -0.17807533370871248\n",
      "step : 80/384 \n",
      "step loss :  -0.16303083398427096\n",
      "step : 100/384 \n",
      "step loss :  -0.18511081982330105\n",
      "step : 120/384 \n",
      "step loss :  -0.20427950325198033\n",
      "step : 140/384 \n",
      "step loss :  -0.21454752370760585\n",
      "step : 160/384 \n",
      "step loss :  -0.2192565915202957\n",
      "step : 180/384 \n",
      "step loss :  -0.2195596803365672\n",
      "step : 200/384 \n",
      "step loss :  -0.22952241637003393\n",
      "step : 220/384 \n",
      "step loss :  -0.25422224281351224\n",
      "step : 240/384 \n",
      "step loss :  -0.2835775287633364\n",
      "step : 260/384 \n",
      "step loss :  -0.30273929160212754\n",
      "step : 280/384 \n",
      "step loss :  -0.2990687362688616\n",
      "step : 300/384 \n",
      "step loss :  -0.29227063110279766\n",
      "step : 320/384 \n",
      "step loss :  -0.2906223618915458\n",
      "step : 340/384 \n",
      "step loss :  -0.2840693501507844\n",
      "step : 360/384 \n",
      "step loss :  -0.28391925913689975\n",
      "step : 380/384 \n",
      "step loss :  -0.2868522160450281\n",
      "EPOCH 4\n",
      "Train Mean loss :  -0.284700276927737\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.27257800841584073\n",
      "step : 40/384 \n",
      "step loss :  -0.3246984304483044\n",
      "step : 60/384 \n",
      "step loss :  -0.23285863037576726\n",
      "step : 80/384 \n",
      "step loss :  -0.2559266498541108\n",
      "step : 100/384 \n",
      "step loss :  -0.28719767813977937\n",
      "step : 120/384 \n",
      "step loss :  -0.3195324271454795\n",
      "step : 140/384 \n",
      "step loss :  -0.3137936439964527\n",
      "step : 160/384 \n",
      "step loss :  -0.31155880576332007\n",
      "step : 180/384 \n",
      "step loss :  -0.30837858794905687\n",
      "step : 200/384 \n",
      "step loss :  -0.3180709444233729\n",
      "step : 220/384 \n",
      "step loss :  -0.3482793159283979\n",
      "step : 240/384 \n",
      "step loss :  -0.3763770552388707\n",
      "step : 260/384 \n",
      "step loss :  -0.3981059781343541\n",
      "step : 280/384 \n",
      "step loss :  -0.3883737900347157\n",
      "step : 300/384 \n",
      "step loss :  -0.3774923536968179\n",
      "step : 320/384 \n",
      "step loss :  -0.37188486884132577\n",
      "step : 340/384 \n",
      "step loss :  -0.35946209796100453\n",
      "step : 360/384 \n",
      "step loss :  -0.35358232648963744\n",
      "step : 380/384 \n",
      "step loss :  -0.35402214813862526\n",
      "EPOCH 5\n",
      "Train Mean loss :  -0.3504500272031073\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.31880712617083834\n",
      "step : 40/384 \n",
      "step loss :  -0.3270143344813037\n",
      "step : 60/384 \n",
      "step loss :  -0.22600206090375016\n",
      "step : 80/384 \n",
      "step loss :  -0.26038811212497687\n",
      "step : 100/384 \n",
      "step loss :  -0.30246974716442754\n",
      "step : 120/384 \n",
      "step loss :  -0.34195260454079374\n",
      "step : 140/384 \n",
      "step loss :  -0.33798553026677136\n",
      "step : 160/384 \n",
      "step loss :  -0.3412344628779984\n",
      "step : 180/384 \n",
      "step loss :  -0.33387340746585853\n",
      "step : 200/384 \n",
      "step loss :  -0.3394555859151768\n",
      "step : 220/384 \n",
      "step loss :  -0.36579495878656276\n",
      "step : 240/384 \n",
      "step loss :  -0.39671514528607\n",
      "step : 260/384 \n",
      "step loss :  -0.41237660975986745\n",
      "step : 280/384 \n",
      "step loss :  -0.4044148934770683\n",
      "step : 300/384 \n",
      "step loss :  -0.3927327792058627\n",
      "step : 320/384 \n",
      "step loss :  -0.38690448795660654\n",
      "step : 340/384 \n",
      "step loss :  -0.37200349126574295\n",
      "step : 360/384 \n",
      "step loss :  -0.3661266488880496\n",
      "step : 380/384 \n",
      "step loss :  -0.36543384140423635\n",
      "EPOCH 6\n",
      "Train Mean loss :  -0.36159969770959277\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.32535216316385945\n",
      "step : 40/384 \n",
      "step loss :  -0.330079879072233\n",
      "step : 60/384 \n",
      "step loss :  -0.2344829364821923\n",
      "step : 80/384 \n",
      "step loss :  -0.272224496382388\n",
      "step : 100/384 \n",
      "step loss :  -0.3185704476475485\n",
      "step : 120/384 \n",
      "step loss :  -0.3674876305531453\n",
      "step : 140/384 \n",
      "step loss :  -0.3648173571155846\n",
      "step : 160/384 \n",
      "step loss :  -0.3696572616657843\n",
      "step : 180/384 \n",
      "step loss :  -0.3617715930907726\n",
      "step : 200/384 \n",
      "step loss :  -0.3715755494964901\n",
      "step : 220/384 \n",
      "step loss :  -0.3985899017508159\n",
      "step : 240/384 \n",
      "step loss :  -0.4280295483133376\n",
      "step : 260/384 \n",
      "step loss :  -0.44814392111215623\n",
      "step : 280/384 \n",
      "step loss :  -0.43685163344153755\n",
      "step : 300/384 \n",
      "step loss :  -0.425077596535053\n",
      "step : 320/384 \n",
      "step loss :  -0.41819312116475904\n",
      "step : 340/384 \n",
      "step loss :  -0.3991633265043048\n",
      "step : 360/384 \n",
      "step loss :  -0.39023966252750514\n",
      "step : 380/384 \n",
      "step loss :  -0.3875662648959673\n",
      "EPOCH 7\n",
      "Train Mean loss :  -0.3831787699665454\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.322930002774044\n",
      "step : 40/384 \n",
      "step loss :  -0.3340279598174408\n",
      "step : 60/384 \n",
      "step loss :  -0.23588906903470763\n",
      "step : 80/384 \n",
      "step loss :  -0.26908303959540597\n",
      "step : 100/384 \n",
      "step loss :  -0.3055120094908642\n",
      "step : 120/384 \n",
      "step loss :  -0.34811401372046724\n",
      "step : 140/384 \n",
      "step loss :  -0.350551362136908\n",
      "step : 160/384 \n",
      "step loss :  -0.363068527428285\n",
      "step : 180/384 \n",
      "step loss :  -0.3617506977565124\n",
      "step : 200/384 \n",
      "step loss :  -0.37092954234040443\n",
      "step : 220/384 \n",
      "step loss :  -0.40383999148884014\n",
      "step : 240/384 \n",
      "step loss :  -0.43513815102306946\n",
      "step : 260/384 \n",
      "step loss :  -0.45146129397243695\n",
      "step : 280/384 \n",
      "step loss :  -0.4451373535781356\n",
      "step : 300/384 \n",
      "step loss :  -0.43307456701463953\n",
      "step : 320/384 \n",
      "step loss :  -0.4239429263646781\n",
      "step : 340/384 \n",
      "step loss :  -0.4024890332494865\n",
      "step : 360/384 \n",
      "step loss :  -0.3917526098138911\n",
      "step : 380/384 \n",
      "step loss :  -0.38811064248080873\n",
      "EPOCH 8\n",
      "Train Mean loss :  -0.3834206047424935\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 20/384 \n",
      "step loss :  -0.31467699998124865\n",
      "step : 40/384 \n",
      "step loss :  -0.33747683537631606\n",
      "step : 60/384 \n",
      "step loss :  -0.24779020726259304\n",
      "step : 80/384 \n",
      "step loss :  -0.2815827187139583\n",
      "step : 100/384 \n",
      "step loss :  -0.32002513073201583\n",
      "step : 120/384 \n",
      "step loss :  -0.35899710579608995\n",
      "step : 140/384 \n",
      "step loss :  -0.3575706859743509\n",
      "step : 160/384 \n",
      "step loss :  -0.36882227069538115\n",
      "step : 180/384 \n",
      "step loss :  -0.37061193465176623\n",
      "step : 200/384 \n",
      "step loss :  -0.379797259003333\n",
      "step : 220/384 \n",
      "step loss :  -0.40930169601633426\n",
      "step : 240/384 \n",
      "step loss :  -0.4419327297531969\n",
      "step : 260/384 \n",
      "step loss :  -0.4663858647368782\n",
      "step : 280/384 \n",
      "step loss :  -0.4542888996061556\n",
      "step : 300/384 \n",
      "step loss :  -0.4448493752811871\n",
      "step : 320/384 \n",
      "step loss :  -0.43569549408731423\n",
      "step : 340/384 \n",
      "step loss :  -0.4118990889310272\n",
      "step : 360/384 \n",
      "step loss :  -0.3988557104779575\n",
      "step : 380/384 \n",
      "step loss :  -0.39328230509746026\n",
      "EPOCH 9\n",
      "Train Mean loss :  -0.3880546912160968\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.31346718312141386\n",
      "step : 40/384 \n",
      "step loss :  -0.342150757928739\n",
      "step : 60/384 \n",
      "step loss :  -0.2585978150751842\n",
      "step : 80/384 \n",
      "step loss :  -0.2956104411099193\n",
      "step : 100/384 \n",
      "step loss :  -0.3366296309586434\n",
      "step : 120/384 \n",
      "step loss :  -0.3727882504882956\n",
      "step : 140/384 \n",
      "step loss :  -0.3694379833616041\n",
      "step : 160/384 \n",
      "step loss :  -0.37943415500756617\n",
      "step : 180/384 \n",
      "step loss :  -0.3817403572823737\n",
      "step : 200/384 \n",
      "step loss :  -0.386034980775691\n",
      "step : 220/384 \n",
      "step loss :  -0.41447911750084515\n",
      "step : 240/384 \n",
      "step loss :  -0.4491721588037602\n",
      "step : 260/384 \n",
      "step loss :  -0.4708113520786105\n",
      "step : 280/384 \n",
      "step loss :  -0.45982593307930436\n",
      "step : 300/384 \n",
      "step loss :  -0.44958310651596145\n",
      "step : 320/384 \n",
      "step loss :  -0.44331979425678264\n",
      "step : 340/384 \n",
      "step loss :  -0.41720215365285307\n",
      "step : 360/384 \n",
      "step loss :  -0.40248918902731157\n",
      "step : 380/384 \n",
      "step loss :  -0.3954154698325674\n",
      "EPOCH 10\n",
      "Train Mean loss :  -0.3896268401288386\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.32697939781451285\n",
      "step : 40/384 \n",
      "step loss :  -0.345949639896624\n",
      "step : 60/384 \n",
      "step loss :  -0.26665701548777593\n",
      "step : 80/384 \n",
      "step loss :  -0.3095490903194971\n",
      "step : 100/384 \n",
      "step loss :  -0.35188600323387154\n",
      "step : 120/384 \n",
      "step loss :  -0.3912718990020131\n",
      "step : 140/384 \n",
      "step loss :  -0.38537636336400705\n",
      "step : 160/384 \n",
      "step loss :  -0.3918508104547917\n",
      "step : 180/384 \n",
      "step loss :  -0.3933198960521025\n",
      "step : 200/384 \n",
      "step loss :  -0.39977700801369\n",
      "step : 220/384 \n",
      "step loss :  -0.4339611820599246\n",
      "step : 240/384 \n",
      "step loss :  -0.4613019499946354\n",
      "step : 260/384 \n",
      "step loss :  -0.48129944309727557\n",
      "step : 280/384 \n",
      "step loss :  -0.4746797163660511\n",
      "step : 300/384 \n",
      "step loss :  -0.46392981306646\n",
      "step : 320/384 \n",
      "step loss :  -0.45227724882772885\n",
      "step : 340/384 \n",
      "step loss :  -0.4249321682349423\n",
      "step : 360/384 \n",
      "step loss :  -0.40909051081407527\n",
      "step : 380/384 \n",
      "step loss :  -0.40063404907484435\n",
      "EPOCH 11\n",
      "Train Mean loss :  -0.39461020803509855\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.30874057828531853\n",
      "step : 40/384 \n",
      "step loss :  -0.3355365262508921\n",
      "step : 60/384 \n",
      "step loss :  -0.25041499449981397\n",
      "step : 80/384 \n",
      "step loss :  -0.3027775166472236\n",
      "step : 100/384 \n",
      "step loss :  -0.351729405594868\n",
      "step : 120/384 \n",
      "step loss :  -0.3972457510577306\n",
      "step : 140/384 \n",
      "step loss :  -0.3919157883045297\n",
      "step : 160/384 \n",
      "step loss :  -0.39715814054104415\n",
      "step : 180/384 \n",
      "step loss :  -0.3963240560528101\n",
      "step : 200/384 \n",
      "step loss :  -0.40030130571433636\n",
      "step : 220/384 \n",
      "step loss :  -0.4362903909247961\n",
      "step : 240/384 \n",
      "step loss :  -0.46422154499688245\n",
      "step : 260/384 \n",
      "step loss :  -0.4872292492064143\n",
      "step : 280/384 \n",
      "step loss :  -0.4776033093582772\n",
      "step : 300/384 \n",
      "step loss :  -0.46984886662057673\n",
      "step : 320/384 \n",
      "step loss :  -0.46006660063608257\n",
      "step : 340/384 \n",
      "step loss :  -0.4332799958190374\n",
      "step : 360/384 \n",
      "step loss :  -0.41735375278580467\n",
      "step : 380/384 \n",
      "step loss :  -0.40863079636944216\n",
      "EPOCH 12\n",
      "Train Mean loss :  -0.402082743410346\n",
      "----------------------------------------\n",
      "step : 20/384 \n",
      "step loss :  -0.28260919961627345\n",
      "step : 40/384 \n",
      "step loss :  -0.31051320165410906\n",
      "step : 60/384 \n",
      "step loss :  -0.22615217060168039\n",
      "step : 80/384 \n",
      "step loss :  -0.2875084878149547\n",
      "step : 100/384 \n",
      "step loss :  -0.3420560615836967\n",
      "step : 120/384 \n",
      "step loss :  -0.3929763491024331\n",
      "step : 140/384 \n",
      "step loss :  -0.3926411931690334\n"
     ]
    }
   ],
   "source": [
    "train(gru, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
